WEBVTT

00:00.000 --> 00:28.680
Hello everyone. Hi, this is George Giaglis. Welcome to week 9 of the University of Nicosia's

00:28.680 --> 00:36.600
free MOOC on NFTs and the Metaverse. Today's topic is trends in visualization technology.

00:36.600 --> 00:45.200
So we are in the second week of addressing metaverse related issues. And as we also did

00:45.200 --> 00:51.480
in the beginning of the course, before introducing NFTs, we discussed about Ethereum and underlying

00:51.480 --> 01:00.160
technologies providing a foundational infrastructure for non-fungible tokens. We are going to do

01:00.160 --> 01:08.160
the same with the metaverse. So today's lecture is going to be more technical in nature. We

01:08.160 --> 01:13.880
are going to be discussing recent trends in visualization. We are going to say augment

01:13.880 --> 01:20.640
it in virtual reality and try to capture how these things fit into the metaverse vision.

01:20.640 --> 01:27.720
And because this is a very specialized topic, I am honored to be joined by two colleagues

01:27.720 --> 01:35.760
who are experts in the space and will be covering the majority of the presentation today. So

01:35.760 --> 01:42.640
without further addon, let me introduce you to the first speaker who is none other than

01:42.640 --> 01:48.440
Chris Christou, a colleague of mine at the University of Nicosia, associate professor,

01:48.440 --> 01:55.800
and head of our VR lab. So Chris, the floor is yours.

01:55.800 --> 02:04.040
Thank you very much, George. Welcome. Welcome to the first part of this session. I am going

02:04.040 --> 02:13.000
to cover 3D rendering visualization and computer graphics in this part. And then my colleague

02:13.000 --> 02:21.880
George will, talk about its uses in virtual and augmented reality. Visualization comes

02:21.880 --> 02:32.640
in all forms. It is pervasive throughout our lives. It is used to render simulations of

02:32.640 --> 02:43.840
architecture of chemical reactions, crowd simulations, fluid dynamics. So computer graphics

02:43.840 --> 02:55.820
is pretty much everywhere. The origins of visualization come from, I guess,

02:55.820 --> 03:04.120
web drawings, but more recently from architecture. So if somebody wanted to, for example, create

03:04.120 --> 03:10.200
a building, they would go to an architect and they would create some drawings for them.

03:10.200 --> 03:17.960
These drawings would be orthographic in nature to preserve parallel lines, to preserve the

03:17.960 --> 03:25.120
shape in order for it to be constructed correctly. And if they got it wrong, they would have

03:25.120 --> 03:33.780
to go back to the client, go back to the drawing board as it were. So this is a long

03:33.780 --> 03:40.800
drawn-up process. This has been replaced by computer-aided design or CAD. Everything is

03:40.800 --> 03:48.320
three-dimensional now. We can walk through a model. We can fly through a model of a building

03:48.320 --> 03:58.240
or a city long before it is even creative. We can also simulate the lighting that is

03:58.240 --> 04:04.760
available in the building at a particular time of day, at a particular location. So things

04:04.760 --> 04:15.000
have changed a lot. Looking forward, we imagine that developments in haptics with auditory

04:15.000 --> 04:22.360
representation or faction even will mean that we don't have just visualization. We will

04:22.360 --> 04:31.280
have perceptualization sometime in the future. These are the enabling technologies that have

04:31.280 --> 04:40.360
helped us along. Primarily the hardware, the graphical processing unit, the GPU, that is

04:40.360 --> 04:46.120
in every device that everyone has in their pockets, in their mobile phones or in their

04:46.120 --> 04:57.840
computers. They can render millions upon millions of polygons per second. And these made it

04:57.840 --> 05:05.320
possible basically for virtual reality and augmented reality to happen. We have high-resolution

05:05.320 --> 05:16.240
displays and this includes the organic LEDs that we have in our VR devices. Computer vision,

05:16.240 --> 05:25.440
AI, machine learning, deep learning are all contributing now to developments in 3D graphics.

05:25.440 --> 05:33.320
So computer vision is responsible. It is the field where you study how to find structure

05:33.320 --> 05:41.840
in the world. Whereas graphics is actually the process of rendering that structure. And

05:41.840 --> 05:52.080
they are forming a happy collaboration. And then finally, we have LiDAR and structure

05:52.080 --> 06:00.760
for motion. These are techniques of finding structure, of representing our real world

06:00.760 --> 06:10.520
and putting this into our computer model. I am going to talk about the history of graphics

06:10.520 --> 06:18.640
first of all and explain some of the processes that go into rendering computer graphics to

06:18.640 --> 06:27.720
give the viewers an idea of what computer graphics is. And then I will end with a few

06:27.720 --> 06:39.200
examples of very recent works. So a brief history of CGI, computer generated imagery.

06:39.200 --> 06:47.680
It was very much influenced by Edwin Kuttma, Pat Hanrahan and Jim Blinn. Edwin Kuttma

06:47.680 --> 06:55.720
was responsible, was one of the co-founders of Pixar which went on to create the short

06:55.720 --> 07:05.120
animation Luxor Junia which is available on YouTube even to this day. And this resulted

07:05.120 --> 07:18.840
in computer graphics being used throughout the movie industries and throughout entertainment.

07:18.840 --> 07:29.280
So these guys were also instrumental in the development of the GPU. And as I just mentioned,

07:29.280 --> 07:37.000
this is what's made everything possible on mobile device, how resolution displays on

07:37.000 --> 07:48.360
mobile devices, computer games on mobile devices and very great games on our PCs. So

07:48.360 --> 07:57.040
behind any graphics is the graphics rendering pipeline. So on the left, on the one side,

07:57.040 --> 08:04.600
you have your application. This is your computer game. This is your VR simulation, let's say.

08:04.600 --> 08:11.400
And you want to get the graphics from that app to the screen on the right hand side.

08:11.400 --> 08:16.920
Okay, so somewhere in there you've got the geometry, you've got whatever it is that's

08:16.920 --> 08:26.880
moving, the zombies that are chasing you. And you want to project that onto the screen.

08:26.880 --> 08:36.120
So that involves various stages of occlusion detection and seeing what is visible from

08:36.120 --> 08:43.560
the screen, working out the colors, et cetera. The rusterization process is the process of

08:43.560 --> 08:50.760
actually drawing something onto the screen. And most of this is done in a scan line order.

08:50.760 --> 08:58.080
So when we talk about scan line, we mean that we refer to the pixels of the screen being

08:58.080 --> 09:04.440
broken up into a rectangular grid. And we usually start at the top left hand side, we

09:04.440 --> 09:09.080
work to the right hand side and we do a zigzag all the way down to the bottom. This is how

09:09.080 --> 09:18.040
we get a 2D image. When we think of the graphics process itself, there is a virtual camera

09:18.040 --> 09:25.240
and there is our geometry. And wherever the virtual camera is, this is what we are projecting

09:25.240 --> 09:31.920
onto the screen. If we're talking about virtual reality, let's say an immersive headset, then

09:31.920 --> 09:37.000
this virtual camera is essentially controlled or moved by your head. So when you move your

09:37.000 --> 09:45.400
head, the virtual camera moves in the virtual environment. A lot of you have heard of ray

09:45.400 --> 09:54.640
tracing. So this is important for a little bit later in what I have to say. So I'll mention

09:54.640 --> 10:02.920
it here. So ray tracing, quite simply, is tracing rays from the eye through each of the pixels

10:02.920 --> 10:10.280
on our screen. And then if these rays don't hit anything, they don't intersect with any

10:10.280 --> 10:15.160
object in the scene, then we just paint the pixel black. If the ray goes through a pixel

10:15.160 --> 10:23.160
and it hits an object, in this case of the point X, then we have to calculate what color

10:23.160 --> 10:32.520
to paint the pixel. Now this color depends on the light. So it's a simple function of

10:32.520 --> 10:38.960
the surface orientation, the surface normal, and we call it at that point, and the angle

10:38.960 --> 10:46.480
that makes with the light source. And this is pretty intuitive. So if the surface is

10:46.480 --> 10:52.600
pointing towards the light, then it gets more energy as brighter. If it's pointing away

10:52.600 --> 11:00.080
from the light source, of course, it receives no illumination and it will be dark. So that's

11:00.080 --> 11:15.800
very simplistic illumination model. So these illumination models made up the core of computer

11:15.800 --> 11:24.440
graphics research in the last, in the early stages of computer graphics. Okay, so these

11:24.440 --> 11:35.280
researchers were busy coming up with models of how to best represent the various effects

11:35.280 --> 11:40.960
that we have in the real world, the various shading effects that we have in the real world.

11:40.960 --> 11:53.160
One of the earliest models is the Fong model. And this can be explained by the diagram at

11:53.160 --> 12:00.920
the bottom left here. So we can break up the illumination of any object, in this case,

12:00.920 --> 12:07.800
this funny looking shape, into three components. The first one is the ambient component, which

12:07.800 --> 12:18.240
is light from everywhere. It adds nothing to the structure, not into the shading. It

12:18.240 --> 12:27.560
just ensures that the whole object is illuminated even though it's not facing the light source.

12:27.560 --> 12:34.440
The next component is the diffuse component or the immersion component. And this does,

12:34.440 --> 12:43.680
as we saw in the previous slide, this is orientation dependent and it adds the shading that you

12:43.680 --> 12:53.800
can see here. And the final component is the specularity, the shiny highlights that you get on

12:53.800 --> 13:04.160
glass and shiny surfaces. So there's a nice representation on the right-hand side where you

13:04.160 --> 13:12.760
can see that the process is not as straightforward as you might have just described as you might

13:12.760 --> 13:20.520
imagine. We have reflection, we have reflection, we have different types of reflection, we have

13:20.520 --> 13:30.360
diffuse reflection, we have specular reflection. So coming up with an illumination model that

13:30.360 --> 13:38.360
actually captures all of this is hard, but the benefits are that you get towards our aim,

13:38.360 --> 13:48.560
which is photorealistic graphics, okay, and post-realism. Another complication is the fact that in the

13:48.560 --> 13:57.840
real world we have indirect illumination and this is nicely portrayed here. So in the image on the

13:57.840 --> 14:08.960
left we have a scene where there is no indirect illumination. On the right we have a scene which

14:08.960 --> 14:16.440
is rendered with global illumination. So let me describe what's going on. The shading patterns

14:16.440 --> 14:24.960
across this image are a function not just of the direct light sources. So this one from the window

14:24.960 --> 14:32.560
for example from here. There's also light bouncing off the floor onto the ceiling, bouncing back

14:32.560 --> 14:41.040
again. So all of this light that's bouncing around in the environment is causing these smith

14:41.040 --> 14:48.720
shading that you can see, illumination of the ceiling essentially which has no direct light

14:48.720 --> 15:00.080
shining on it. So things are not as simple as we would hope in the real world. More about that

15:00.080 --> 15:11.760
later. So let me describe now just a content generation which is the stuff, the geometry, the

15:11.760 --> 15:20.000
stuff that's actually in our computer game that's in our television commercial. If it's 3D it's

15:20.000 --> 15:27.360
going to have been made in some 3D editor. This is the interface with 3D Studio Max. The first thing

15:27.360 --> 15:36.000
to note is that everything is polygonized. Everything consists of polygons. They use the flaps,

15:36.000 --> 15:43.760
flat simple surfaces. We join them all together and not one by one but we join them all together

15:43.760 --> 15:50.400
to make curved surfaces. On the right you may see you may be able to make out that there are basic

15:50.400 --> 15:57.120
primitive shapes and boxes for example and spheres and these are used to create more complicated

15:57.120 --> 16:03.920
objects. You may also note the teapot and this is just the Utah teapot. I've put a link there. It's

16:03.920 --> 16:12.000
a very special teapot that's been used for computer graphics research for the last 40 or so years.

16:15.120 --> 16:22.880
Characters, avatars, character modeling. There's no difference here. They still consist of polygons.

16:23.920 --> 16:32.640
The special thing about 3D characters or avatars is that they have a biped rig or a biped skeleton

16:32.640 --> 16:40.640
which is the actual thing that does the animation. If we're talking about animated games for example

16:42.160 --> 16:49.120
somebody has to create the animations and piece the animations together. This can be done with

16:49.120 --> 16:56.400
keyframing or it can be done with motion capture where a real actor performs the motions and then

16:56.400 --> 17:05.920
these motions are used to move the virtual character. At the bottom just a brief mention

17:05.920 --> 17:14.160
about this we can do crowd simulation. This is something from my own work. So we're simulating how

17:15.280 --> 17:20.400
annoyed people get with measuring how annoyed people get when they're surrounded by crowds.

17:20.400 --> 17:29.040
But you can also use it for escape route planning for example to simulate what happens when there's

17:29.040 --> 17:37.680
a fire in a building. This is a multi-character scenario where we've got many non-player characters

17:37.680 --> 17:46.560
in the scene. Again these are no different from the character models that I mentioned previously.

17:46.560 --> 17:55.920
Probably just a glare resolution. We're looking at current trends now in the last few slides.

17:57.360 --> 18:10.000
LIDAR is used throughout for measuring distance for getting a structure or a special structure.

18:10.000 --> 18:17.680
The principle here is the same as an echo. It takes a while for light to

18:19.280 --> 18:25.600
projected light to bounce back from surfaces. So we can measure the time that it takes for light

18:25.600 --> 18:34.720
to come back to an emitter. In this case it's available. We're on consumer devices like the iPad Pro.

18:34.720 --> 18:47.440
From this we get a pixelated version of the image in front of us. So a point map.

18:48.080 --> 18:54.320
And the point map can be turned into a depth map which is just an encoding of how far

18:55.200 --> 19:02.560
objects are away from us relative depth. In turn this depth map can be turned into

19:02.560 --> 19:12.880
a structure and a 3D model. This is used throughout the modern tech that you will hear about later on.

19:12.880 --> 19:23.440
So all of the devices that for example the meta-quest that uses this to work out where it is in the

19:23.440 --> 19:32.000
room and all the augmented reality and glasses use this to work out the surfaces on which to

19:32.000 --> 19:38.720
project their graphics. This is some exciting work that's done by meta.

19:39.840 --> 19:49.760
For me known as Facebook. So here they're actually getting the structure of human beings, of people.

19:50.320 --> 19:54.880
So people would go in here and they would have their head scanned or their body scanned. So this

19:54.880 --> 20:01.760
is a multi-camera rig and a multi-light rig just to ensure that there are no shadows.

20:03.520 --> 20:12.880
It's used to extract the structure or somebody's face in this case or also the textures of the

20:12.880 --> 20:23.840
face. And then deep learning can be used to reconstruct expressions that a device such as the quest

20:23.840 --> 20:30.720
for example of the future quest text that the user is making. So if somebody is grimacing

20:30.720 --> 20:34.480
then their avatar will be grimacing in the metaverse.

20:38.160 --> 20:44.560
If you have a, so and light fields and talk about light fields, if you have a steam

20:44.560 --> 20:53.920
account or a HTC Vive or MetaQuest it's worth downloading. Google's welcome to light fields.

20:54.720 --> 21:01.440
So previously I was talking about getting the structure of the person, now you're getting the

21:01.440 --> 21:13.440
structure of the environment. And this is a wonderful demonstration of how realistic graphics

21:13.440 --> 21:23.600
can be. So what Google have done here is they've mounted a number of GoPro cameras onto this

21:23.600 --> 21:33.280
rotating rig and they're basically sampling the amount of light in the room. And as the rig is

21:33.280 --> 21:40.240
rotating around the calculating, they're sampling the light structure of that room, the so-called

21:40.240 --> 21:49.280
light field. Okay, if you store this you can play it back to somebody. And then the feeling is,

21:49.840 --> 21:56.880
is not just one of her realism but it's also a way to capture the specular

21:58.080 --> 22:06.880
parts, the specular components of the surfaces in your scene, shiny surfaces, etc.

22:06.880 --> 22:15.760
Light fields that I'm in particular, I would not mention too much about this as we are,

22:15.760 --> 22:25.200
we need to progress. But if you want to capture the full extent of light in the scene, you really

22:25.200 --> 22:34.640
need to use a multi-camera grid. I'm showing in this middle diagram. You can also use a

22:34.640 --> 22:44.080
plenotic camera. So in the Google case, they would use the multiple GoPro's which were rotating.

22:45.120 --> 22:51.760
But if you want a forward-facing camera, you can use a plenotic camera,

22:53.440 --> 23:00.640
which as you can see takes small images of the same scene and you can put these together

23:00.640 --> 23:10.240
in order to get motion parallax, in order to get the shine from a glass, for example,

23:12.240 --> 23:19.200
in order to see a little world or represent the world more realistically.

23:21.440 --> 23:30.240
Now, if you had a limited number of these samples, you could use a neural network to

23:30.240 --> 23:37.760
actually calculate or to represent the space in between. So this is the principle of NUFs.

23:39.440 --> 23:48.880
And this is the very recent paper by Benjamin Attel from this year. And this demonstrates,

23:48.880 --> 23:59.520
first of all, the power of occlusion effects in depth perception, the power of motion parallax,

24:01.600 --> 24:15.120
the realism and specularities, and just the overall structure. So these are novel views of

24:15.120 --> 24:26.080
just a limited number of samples of images. This is the basic principle of NUFs. So the idea is

24:26.080 --> 24:35.280
that you take a small subset of images from an object and you create a volumetric representation

24:35.280 --> 24:46.720
using a convolutional neural network. This is another demonstration of this from NVIDIA's

24:46.720 --> 24:55.040
websites. Look, put a link to this down below. So again, you have limited number of views

24:55.040 --> 25:08.720
and the fly through, which is generated by the neural network. So the thing that you might be

25:08.720 --> 25:15.280
thinking at this stage, and especially seeing the image on the right hand side, is can we get an

25:15.280 --> 25:23.120
actual model out of this? Can we get a computer model out of this three-dimensional neural network

25:23.120 --> 25:31.920
representation? And the answer is yes, and people are actually working on this. So again, this is

25:31.920 --> 25:43.360
very recent work by Munkberg taking more to view images representing these within a neural network

25:43.360 --> 25:54.000
and outputting a mesh, a three-dimensional mesh of the object, as well as the textures that are used

25:54.000 --> 26:01.280
to paint the detail onto the surface, and also the light probes. So the light probes capture the

26:02.480 --> 26:08.320
specular components, and this can be output straightened to your favorite game engine, Unity,

26:08.320 --> 26:18.160
for example, Unreal or into a 3D editor where they can be further edited.

26:19.440 --> 26:29.360
Okay, so that brings me to the end of what I wanted to introduce you to in my segment. So

26:29.360 --> 26:40.080
we'll pass over to my colleague, George, and he will take over. Thank you very much, Chris.

26:41.040 --> 26:49.360
That's some fascinating stuff here. So for those of you, the students, I mean, who are interested

26:49.360 --> 26:57.760
in this, and what George Kootetas is going to present in a while, let me tell you that we're

26:57.760 --> 27:06.000
working towards creating a follow-up course that will focus on the upcoming developments in terms of

27:06.000 --> 27:12.080
hardware and goggles and masks and headsets and all that stuff. There's some truly fascinating

27:13.520 --> 27:20.320
developments happening by some big companies and some startups. So we will revisit the space,

27:20.320 --> 27:25.280
post Christmas, and have a special day's course for those that are interested in seeing how our

27:25.280 --> 27:31.600
world will change. So Chris will stay with us until the end, and he will be available for questions

27:31.600 --> 27:40.480
if you have any. But before we go to the Q&A, let me introduce our other speaker for the day.

27:41.280 --> 27:49.120
My colleague and friend, George Kootetas, George is an executive and entrepreneur and academic

27:49.120 --> 27:57.120
with more than a decade of experience in business and R&D. He has a multicultural background. He

27:57.120 --> 28:03.920
has spent six years in Austin, Texas, five years in the UK, and another six years in Greece,

28:03.920 --> 28:10.960
where he's currently based. He has founded a startup company in Austin working on AR and VR

28:10.960 --> 28:17.360
and training of first responders and has a number of publications at the end of the patent in the

28:17.360 --> 28:24.720
AR and VR space. So it's a great pleasure for me to introduce him to the course. George, are you with us?

28:26.960 --> 28:30.800
Yes, hello everyone. Thank you, George, for the warm welcome.

28:33.360 --> 28:38.080
Hello, everyone. Can you hear me? I hope you can.

28:40.560 --> 28:47.040
George, thank you very much for your warm welcome. Just give me an indication that you can hear me

28:47.040 --> 29:04.240
so I can continue. Okay, all right. So I'm very happy to be here with you today and speak to you

29:04.240 --> 29:09.840
and introduce you to the concept of extended reality. Some of the things that we're going to

29:09.840 --> 29:16.800
discuss today, probably you might be already aware, but some of them might be new to you and

29:16.800 --> 29:25.760
might help you expand your horizons. So we all hear about VR, AR, mixed reality.

29:27.440 --> 29:35.040
Let's understand the difference. Virtual reality, a simulated experience, okay, in a fully virtual

29:35.040 --> 29:42.800
world, and this is available to you through a 3D near-eye displays. So you are fully isolated

29:42.800 --> 29:48.240
from the physical environment. You are in a totally virtual environment with graphics presented to

29:48.240 --> 29:56.240
you from a display in front of your eyes. On the other hand, augmented reality allows you to interact

29:56.240 --> 30:05.680
with the physical world and it overlays digital information and content on top of the physical world.

30:05.680 --> 30:13.680
Mixed reality, which is now, you know, sometimes we use the same term augmented and mixed reality,

30:13.680 --> 30:22.480
is the ability of the digital content to interact with the physical environment. So this means that

30:22.480 --> 30:28.880
if you can see the third circle, the 3D graphic is behind the sofa and it is in the Sado region of

30:28.880 --> 30:36.320
the sofa. The sofa is a physical object in my living room and the robot is a digital content

30:36.320 --> 30:42.400
and I can partially see it. This is called mixed reality. Augmented reality, an example of augmented

30:42.400 --> 30:51.520
reality was Google glasses, okay, or even our smartphones that we can have AR applications.

30:51.520 --> 30:57.120
Mixed reality is more modern applications that can be made usually with Microsoft HoloLens and

30:57.120 --> 31:06.320
other AR devices. From now on, just to not get confused, AR, MR can be thought of almost the same.

31:09.040 --> 31:17.280
In order to experience AR, VR, we need to have a head-mounted device and as you already know,

31:17.280 --> 31:26.160
there is a plethora of devices in the market. The breakthrough in the HMD head-mounted device came

31:26.160 --> 31:34.640
from Palmer Lackey in a Kickstarter project. This was in 2012, I think, but started the Oculus, okay,

31:34.640 --> 31:41.200
and there was an excitement there and then an angler of the technology because many developers

31:41.200 --> 31:48.880
used the development kit, DK1, offered by Oculus. So they were able to create applications in the

31:48.880 --> 31:58.640
VR space and people can access them through a marketplace. In the image here on the left hand,

31:58.640 --> 32:05.680
you can see some VR headsets. One is the Oculus Rift. You can see a cable because it required to

32:05.680 --> 32:13.280
be connected to the computer for some processing power. You can see the cardboard that you put your

32:13.280 --> 32:22.000
smartphone in order to act as the VR display and also you can see the latest versions of meta

32:22.000 --> 32:29.440
Oculus Quest. On the right side, you can see some, a couple of examples of AR headsets. We have

32:29.440 --> 32:38.320
Google Glasses, Microsoft HoloLens and Magic Clip. You can see some of the content here. I'm not

32:38.320 --> 32:42.960
going to read out to you, obviously. I'm explaining in the images. Feel free to use the slides and,

32:42.960 --> 32:53.280
you know, dive a little bit deeper in the terms. So we have AR VR experiences deployed to us through

32:53.280 --> 32:59.760
head-mounted devices. What are the applications? There are numerous applications that we can

32:59.760 --> 33:07.920
experience. Both AR VR has a little bit of struggle in finding the key application areas.

33:08.640 --> 33:15.840
So we have seen VR going very deep in the gaming space. But then other application areas may involve

33:15.840 --> 33:21.920
learning and development, remote collaboration, social networks, or even industrial applications.

33:21.920 --> 33:29.600
AR is the same. But we will see as the time passes that VR is more like on the gaming aspect

33:30.640 --> 33:37.280
and remote collaboration and social networks, whereas AR can be used mainly for industrial

33:37.280 --> 33:42.080
manufacturing, construction applications, or learning and development because it allows us to

33:42.080 --> 33:48.160
interact with the physical environment. This is not 100% true. Obviously, we have VR applications

33:48.160 --> 33:52.800
since they're learning and development or industrial applications. But we have seen these separations

33:52.800 --> 33:58.080
on the application areas. And obviously, the reason is that AR allows you to interact with

33:58.080 --> 34:05.760
the physical world. Gaming is huge. By 2024, it's going to be 2.5 billion.

34:08.000 --> 34:11.040
Remote collaboration, we have companies like Special I.O.

34:11.040 --> 34:20.400
Agriculture, it is a recent trend in the integration of AR with Internet of Things.

34:20.960 --> 34:26.480
So we already have augmented reality startup companies that help farmers

34:27.200 --> 34:36.240
personalize, actually optimize the quality of the growth of their fields by either deploying

34:36.240 --> 34:42.480
sensor networks and taking measurements of the humidity, etc., or by using smart cameras

34:42.480 --> 34:49.600
that allows to optimize where you need to put more water, etc. So there are very, very exciting

34:49.600 --> 34:57.920
applications. Learning and development, as we will see a little bit later, VR and AR has a very

34:57.920 --> 35:06.880
important advantage compared to traditional, let's say, web training programs. It improves

35:06.880 --> 35:12.480
cognitive learning, but also muscle memory, because you're moving your hands, you can move

35:12.480 --> 35:17.920
in the environment and the brain can remember where items are positioned and what actions you need

35:17.920 --> 35:26.560
to do if it is related to a repetitive work. So very fascinating. Obviously, health, we have a lot

35:26.560 --> 35:31.040
of applications in the health sector, either in the training but also during operation.

35:33.520 --> 35:40.320
Manufacturing industrial, you don't need to be an expert in order to do a repair. You can download

35:40.320 --> 35:45.520
the instructions and you can do the repair at the same time that you are actually doing the repair

35:45.520 --> 35:52.320
of a machinery or etc. We are not very far away of what we have seen in the movies that you can

35:52.320 --> 36:00.160
download something in your not brain, on your AR device and execute it without being an expert

36:00.160 --> 36:06.720
in the field, similar to matrix. Architecture and construction, obviously there are numerous

36:06.720 --> 36:14.880
applications there. So the world is fascinating and ARVR will definitely be dominating in our lives

36:14.880 --> 36:23.200
and our work now and in the future. It's really interesting to see how this technology was

36:23.200 --> 36:33.440
evolved. The first HMD had the mount and device started in 1943. Yes, believe it or not, it's so

36:33.440 --> 36:39.440
old. You can see that there was a big gap. Obviously, the technology was not there, the user adoption

36:39.440 --> 36:47.920
was not there. Then suddenly in 1960 to 1969, there was a decade of people with the growth of

36:47.920 --> 36:54.480
the computers. They started experiencing different types of technologies in order to

36:54.480 --> 37:00.240
create immersive environments. The most exciting milestone was in 1962 with

37:01.520 --> 37:06.960
an immersive experience called Saint Sorama. If you see the video now, it's going to be funny,

37:06.960 --> 37:16.480
but for 1962 it was a breakthrough. You will see that there are waves, bursts, let's say, of

37:16.480 --> 37:24.640
evolutions. Now we are in the time that the technology, the hardware, is there. We have portable devices

37:24.640 --> 37:32.080
with very great quality of experience and quality of the graphics. The time is now

37:32.080 --> 37:42.560
to exponentially grow the sector. In the VR space, there are a lot of companies that provide

37:44.000 --> 37:52.560
head-mounted devices. Obviously, one of the most well-known is Metacoculus. They bought a company

37:52.560 --> 37:59.760
some years ago and they focused initially on the gaming aspect and they had more like a B2C

37:59.760 --> 38:04.320
approach, business to consumer. They addressed the consumer market and there was the first let's say

38:07.040 --> 38:12.320
exponential adoption of the device. Obviously, there are other companies out there like Google,

38:12.320 --> 38:22.480
HTC Vive, Samsung, etc. Remember, in the VR space, it all started with Saint Sorama.

38:22.480 --> 38:33.200
I highly recommend to see this video to understand how 60 years ago people created the first immersive

38:33.200 --> 38:41.680
experience. If we focus on one product, the most famous, let's say Oculus Quest, you will see that

38:41.680 --> 38:49.440
it started with a passive VR experience without any type of controllers. It was more like visualization.

38:49.440 --> 38:56.240
Then we had the Oculus Rift that was connected to EPC in order to provide some required processing

38:56.240 --> 39:05.280
power. Then we had Go and Oculus Quest that they work with battery and in a standalone manner,

39:05.280 --> 39:12.480
so you don't need to connect it to a computer. Then we had Metacwest Pro that was recently announced

39:12.480 --> 39:19.120
that it reduces very cool features like mixed reality. You can see that in the front part of

39:19.120 --> 39:24.400
the display, there are cameras that allow you to perform gestures and you can use your actual hands.

39:24.400 --> 39:33.040
You don't need the joystick. The level of experience and the graphics has dramatically improved

39:33.040 --> 39:36.720
compared to different versions. This evolution is met in all companies.

39:36.720 --> 39:45.200
In the AR space, we have also a lot of companies that provide devices. The most famous one is

39:45.200 --> 39:56.400
Microsoft HoloLens, Magic Clip. Metac is still present in the AR space with what they call Spark AR.

39:56.400 --> 40:02.960
It's a platform that anyone can create AR experiences that are used on the mobile device. They don't have

40:02.960 --> 40:08.080
an AR headset yet, at least available in the market.

40:12.400 --> 40:20.560
Magic Clip is an important company to see because there was an initial hype back in 2014. I think

40:20.560 --> 40:29.680
they raised a lot of money for a huge amount of evaluation. The company was not ready to provide

40:29.680 --> 40:35.360
the product and didn't address the rightness market to penetrate in the market.

40:36.000 --> 40:40.160
That's why there was more like an idle mode for this startup company.

40:40.160 --> 40:47.360
But recently, we see a lot of motion and evolution coming from Magic Clip since they

40:47.360 --> 40:54.720
trimmed their business model to more like enterprise AR and use cases related to health.

40:54.720 --> 41:00.960
So we expect to see a lot of growth and a lot of cool new features from Magic Clip too.

41:04.400 --> 41:12.960
Obviously, a recent trend is coming from the Metavors. Imagine we have AR, VR companies,

41:12.960 --> 41:17.360
we have computer graphics companies and now we have companies in the Metavors space.

41:18.160 --> 41:24.160
Either by creating 3D environments, either by creating serious games and interactive

41:24.160 --> 41:37.120
environments like Roblox Corporation, Descendralant etc. It's going to be fast to see what type of

41:37.120 --> 41:43.280
collaboration, accusations or merges are going to happen between the AR, VR and the Metavors space.

41:43.840 --> 41:49.680
So I'm sure that in the next years we're going to see a lot of action in this space.

41:49.680 --> 41:59.040
But let's see what is happening inside a headset. What is inside? What type of electronics do they

41:59.040 --> 42:05.440
have? Obviously, these bullet points do not represent the entire technology but they can give you a

42:05.440 --> 42:14.560
good high level overview of what exists and what are the main components. This is the device from

42:14.560 --> 42:21.920
Meta, MetaQuest Pro. There are front cameras, depth cameras in order to understand proximity

42:21.920 --> 42:28.160
and gesture tracking. So you can put your hands in front of the cameras and by moving your fingers

42:28.160 --> 42:36.240
you can see your virtual hands moving with great accuracy. There are also high tracking sensors

42:36.240 --> 42:45.440
which are important especially when you do like social interaction with another person and the

42:45.440 --> 42:50.800
other person can see your eyes or by optimizing the graphics and the frame rate according to the

42:50.800 --> 42:59.120
place that you focus your eyes. There are devices called IMUs, Inertial Measurement Unit, accelerometers,

42:59.120 --> 43:08.480
orientation and other gravitational forces and include accelerometers, gyroscopes and magnetometers.

43:09.040 --> 43:15.200
They are used for you to accurately measure position of your hands or the rotation of your head.

43:16.400 --> 43:23.600
Time of light sensors in order to measure distance, imagine you are entering a room and this physical

43:23.600 --> 43:30.320
room that you enter can automatically become a virtual room in your virtual reality experience.

43:30.320 --> 43:36.320
So you need depth cameras and time of light sensors to do that. There are processors,

43:36.320 --> 43:40.000
speakers, battery obviously and controllers.

43:44.240 --> 43:51.200
The controllers of the VR are quite interesting to observe because if you think the user experience

43:51.200 --> 43:58.400
before VR you have controllers of game consoles. You use both your hands in one device but now

43:58.400 --> 44:03.600
in virtual reality you can actually physically move so you cannot have one controller for both

44:03.600 --> 44:13.840
of your hands. So the UX of every company out there was responsible to convert the controllers

44:13.840 --> 44:21.120
that we had in the gaming consoles to two separate controllers with additional sensory device on

44:21.120 --> 44:26.800
top of them accelerometer gyroscopes in order to simulate the movement of our hands and provide

44:26.800 --> 44:32.640
the required user experience for us to interact and play our virtual reality games.

44:33.840 --> 44:41.280
And this graph shows how these two different companies created two different joysticks coming

44:41.280 --> 44:49.600
from the concept of the console joystick. But when you don't have a controller

44:51.520 --> 44:58.400
you need to have gestures in order for the device to understand where are your hands

44:59.040 --> 45:05.200
and what are the motions of your fingers. And this is achieved both in AR devices and in the

45:05.200 --> 45:12.960
AMV devices with the cameras that are in front of the headset. So these cameras have the required

45:13.520 --> 45:18.720
let's say algorithms that power the cameras to understand the motion of the fingers

45:19.760 --> 45:26.240
and according to the different type of motions that you do you can interact with a virtual

45:26.240 --> 45:31.360
environment. So for example when you do in a Microsoft HoloLens this movement which is the

45:31.360 --> 45:37.840
movement called Bloom the main menu appears. If you want to click you need to do this with your

45:37.840 --> 45:44.800
finger not this this is the gesture. If you want to drag and drop something you click it and you

45:44.800 --> 45:53.120
drop it. So there are different types of gestures in order to allow you to interact with a virtual

45:53.120 --> 46:06.640
environment in an AR or in a VR equipment. What type of delivery mechanism and technologies do we

46:06.640 --> 46:14.560
have in order to experience AR VR? There are numerous let's go each one of them. Where they are

46:15.440 --> 46:21.040
where they are is a virtual reality experience but it is deployed on the browser of your laptop

46:21.040 --> 46:28.720
or your computer or wherever you want. Obviously you don't have all the nice features of VR

46:29.520 --> 46:37.360
it feels like you are playing a 3d game okay but it might be the right solution according to the

46:37.360 --> 46:43.520
program and the application area. So for example if you want to create a training program for students

46:43.520 --> 46:49.520
or people to get familiar with the space and what VR might be the right place to deploy your

46:49.520 --> 46:57.200
experience because it's already available anywhere everybody has a browser okay. It's very cheap

46:57.200 --> 47:03.040
you don't need to buy any new equipment. On the other hand if you need to create a more immersive

47:03.040 --> 47:09.120
environment like a game or a more immersive training then you need a full VR experience

47:09.120 --> 47:18.800
and deploy your experience on the VR headset. Obviously the web VR is cheap also the VR cardboard

47:18.800 --> 47:25.360
it's cheap because the device the cardboard is almost for free it's already it's very cheap

47:25.360 --> 47:33.440
and you only need the smartphone. On the AR space you can deploy your AR application on a smartphone

47:34.400 --> 47:40.240
I'm sure that you all played Pokemon Go or I'm sure you're playing now AR games on your smartphone

47:41.040 --> 47:46.480
they can be deployed on smart glasses either Google glasses that I'm not sure if who of you

47:46.480 --> 47:55.120
experienced that in the past I tried them back in 2015. We now have car manufacturers having smart

47:55.120 --> 48:01.600
glasses in front of the wheel of the car in order to inform the driver about you know navigation

48:01.600 --> 48:08.400
or specific alert and obviously we have AR headset but you can deploy your AR applications like

48:08.400 --> 48:14.640
HoloLens, Magic Leap etc. So according to the application the level of immersion and the use

48:14.640 --> 48:21.440
case you have a plethora of delivery mechanisms delivery technologies for your AR VR experiences.

48:25.280 --> 48:31.840
Something that is interesting also in the AR VR space is optics so in order to make the experience

48:31.840 --> 48:42.160
even more immersive we now have gloves that can have sensory devices in order to improve the

48:42.160 --> 48:48.720
overall experience so vibration so your finger vibration on a shoot that you are wearing.

48:49.360 --> 48:55.520
So imagine that you are playing let's say a game that you are giving a punch to the enemy and you

48:55.520 --> 49:02.000
can feel the punch on your chest or you are in a forest and you can see a bird flying or

49:03.120 --> 49:10.000
the bird is landing on your finger and you can feel it. So all this extra level of immersion

49:10.000 --> 49:17.440
is delivered to you through extra hardware equipment that are obviously in a tactile

49:17.440 --> 49:24.240
monitor communicating really fast with the hair with a VR headset and you need to have some extra

49:24.240 --> 49:34.240
hardware to experience it. There are other level of immersion in order to have a better VR experience

49:34.240 --> 49:41.440
this is an example one of the most famous is the treadmill that allows you to run in VR this was

49:41.440 --> 49:46.000
one of the main drawbacks of VR compared to AR in AR you can move your hands but also you can

49:46.000 --> 49:50.960
move your body in virtual reality you can move you cannot move your body you only have the joy

49:50.960 --> 49:58.320
stick in order to navigate in the environment with treadmills via treadmills you are on top

49:58.320 --> 50:08.480
of a treadmill you can run you can do all the physical movements and these are translated as

50:08.480 --> 50:18.880
locomotion in the VR space we also have flying simulators we can have theme parks etc in order

50:18.880 --> 50:27.920
to increase the level of immersion hey I'm not gonna spend too much time on two of the most

50:27.920 --> 50:34.080
let's say commonly used engines to create AR VR experiences you know I'm sure that you are

50:34.080 --> 50:41.280
all familiar with unity and unreal both of them are engines that allows you to create a VR and

50:41.280 --> 50:50.320
AR experience as a very general rule of thumb unreal engine is most widely used in games

50:50.320 --> 50:59.440
it has very good graphics where else unity has a lot of libraries that can help you if you want to

50:59.440 --> 51:07.760
create it more like trainings and other type of VR experiences but obviously this is not a no

51:07.760 --> 51:15.120
a hard rule it is quite commonly met out there so if you are a startup and you want to create let's

51:15.120 --> 51:24.080
say if not a higher resolution graphic VR experience but more related to training and learning and

51:24.080 --> 51:28.720
development unity might be the right tool because there are a lot of languages out there and libraries

51:29.520 --> 51:36.720
if you want to create a very realistic game then probably unreal engine might be the right platform

51:36.720 --> 51:46.160
for you but obviously depends on you know the use case and the application so now let's move to

51:46.160 --> 51:54.400
some of the development challenges that we face nowadays how do you develop a VR experience

51:55.680 --> 52:01.600
most probably you are aware of agile development process so let me give you some personal

52:01.600 --> 52:09.520
some some I'm gonna give you some of the lessons I personally learned in my startup career

52:10.400 --> 52:20.960
creating a VR or an AR training or game let's say experience is time consuming and quite a

52:20.960 --> 52:27.120
difficult thing to do this is because there is a plethora of platforms that is a plethora of

52:27.120 --> 52:36.720
devices you can use there is a plethora of different type of 3D objects and environments that you can

52:36.720 --> 52:42.080
create and in most of the cases what I have seen is that you don't know what really the customer

52:42.080 --> 52:52.800
wants the user so one of the most commonly we help you on your development of the experience is what

52:52.800 --> 52:59.280
we call agile development process that helps you understand what the user needs and what are the

52:59.280 --> 53:06.080
challenges explore the different alternatives you have experiment and then materialize and this is

53:06.080 --> 53:13.200
done through interactive cycles with small cross-functional teams so instead of going and creating a

53:13.200 --> 53:20.480
monolithic game or experience that nobody's gonna use it try to make it a adaptive and iterative

53:20.480 --> 53:27.200
so we wanted to create a virtual reality training for first responders and this virtual reality

53:27.200 --> 53:34.240
train should be delivered in virtual reality a oculus quest and also a are experience using

53:34.240 --> 53:40.240
Microsoft HoloLens so we need to develop two products but we didn't even know what the user

53:40.240 --> 53:46.480
and the customer wanted so for example and design thinking principles what we did is we

53:46.480 --> 53:55.600
created an MVP with 360 images or 360 videos and we use InstaVR as a platform to let users

53:55.600 --> 54:02.400
experience it very easy to do and to tell you the truth the budget that you need is less than 500

54:02.400 --> 54:10.000
dollars let's say or euros or zero amount of money you'd go in the place you take 360 images and

54:10.000 --> 54:16.720
then you program in an InstaVR and experience you give it to the users and you receive a feedback

54:16.720 --> 54:22.560
I would like this feature I don't like that I would like to add another feature so with this

54:22.560 --> 54:32.560
iterative process you know we started creating progressively experiences in VR and AAR we published

54:32.560 --> 54:41.680
that on a VR store and then we were able to scale it to a large number of users I definitely want

54:41.680 --> 54:49.360
to give you this advice that don't go and develop something big focus on an MVP MVP stands for a

54:49.360 --> 54:56.480
minimum viable product and follow agile principles iterative work in order to you know step by step

54:56.480 --> 55:07.600
improve your model and your experience if you want to to see all the development stages of an AR and VR

55:07.600 --> 55:14.880
experience you know the most basic steps are the following create the theory environment design

55:14.880 --> 55:20.880
create the instructional design let's say the series game and the experience behind it create

55:20.880 --> 55:28.080
some special effects and immersion levels you know some special gestures define what are going to be

55:28.080 --> 55:35.200
the analytics that you need to keep track in order to understand user engagement package all of these

55:35.200 --> 55:42.480
in an application file and publish it on a marketplace on every step there are a lot of questions that

55:42.480 --> 55:48.960
you need to answer these are just a small tiny portion of the actual questions that exist out there

55:48.960 --> 55:55.520
but it gives you let's say an indication of what are the steps involved what are the main let's say

55:56.480 --> 56:01.280
obstacles that you need to bypass in realities 10x of what you see here

56:03.360 --> 56:10.720
another development challenge is the avatar who owns my avatar what type of diversity we need to

56:10.720 --> 56:18.000
give to people it needs to be customizable I want to have my face on the avatar some other people

56:18.000 --> 56:26.400
want to be anonymized or wear sunglasses so giving the creating a avatar is not a simple thing in

56:26.400 --> 56:33.600
modern AR and VR experiences and it is something that is gonna we are gonna see a lot of innovation

56:33.600 --> 56:42.880
in the near future another challenge that we met in mainly in virtual reality is what we call motion

56:42.880 --> 56:50.160
sickness it's an important drawback because I personally experience it sometimes because it

56:50.160 --> 56:58.960
doesn't let you experience the entire virtual reality game after five minutes or ten minutes you

56:58.960 --> 57:06.560
might feel motion sickness and you might quit a bond on the game it's quite interesting to see

57:06.560 --> 57:18.320
how motion sickness is created so we have two sensors that detect motion in our body one is our

57:18.320 --> 57:28.400
ear and the other is our eye inside our ear there are some tiny tiny tiny sensors that understand you

57:28.400 --> 57:34.960
know motion think of it like an accelerometer inside our ear okay and obviously the eye detects

57:34.960 --> 57:45.280
motion through the visual when we experience VR what is happening is that the brain that is

57:45.280 --> 57:53.360
connected to our ear and our eye receive two signals that are opposite the ear does not feel

57:53.360 --> 58:00.480
any type of motion and it sends a no signal motion to the brain world the eye can see the motion

58:00.480 --> 58:05.600
because I can see you know motion in the virtual reality environment cars are passing by you know

58:05.600 --> 58:12.800
I'm flying a plane and the brain does not know which of these two sensors to trust more because it

58:12.800 --> 58:20.960
has an equal trust to both of them it trusts the ear it trusts the eye so in order to defend itself

58:20.960 --> 58:28.640
the brain sends a sickness signal to our stomach and this forces to stop whatever we do that creates

58:28.640 --> 58:36.400
motion sickness was so recent trend now in head-mounted devices in VR headsets is that

58:36.400 --> 58:45.280
it's going to include a magnetic sensor actually an actuator on the ear side in order to synchronize

58:45.280 --> 58:53.920
the motion that the eye detects with an actuator on a rear in order to also detect a fake motion

58:53.920 --> 59:01.680
so motion sickness is something that it's not going to happen from now on in many of the new VR headsets

59:04.160 --> 59:11.120
another cool challenge that is happening is what we call teleportation it's not like actual

59:11.120 --> 59:18.720
teleportation but it's very similar to what we have seen in Star Wars movie the idea for teleportation

59:18.720 --> 59:27.360
is for me to be able to see a 3D full-scale avatar of the person that I'm communicating with

59:27.360 --> 59:35.040
so imagine that I'm in my room yeah you are in your room and you can see my 3D body walking inside

59:35.040 --> 59:41.120
your room and delivering you this lecture there are different types of technologies to do that

59:41.120 --> 59:49.680
either by transferring a large number of pixels in this 3D environment or by creating a 3D object

59:49.680 --> 59:57.760
and setting putting a skin of how I look on top of it obviously there are different types of

59:57.760 --> 01:00:04.560
cameras and hardware equipment that need to be created I'm not an expert about that but I definitely

01:00:04.560 --> 01:00:12.960
know that there are a lot of development challenges in that teleportation space and before I close

01:00:12.960 --> 01:00:20.720
another challenge is how we interact with all these huge networks of internet of things that are out

01:00:20.720 --> 01:00:31.040
there imagine that by 2025 or it might already be happening you know non-human centric data

01:00:31.040 --> 01:00:36.560
data that are coming from internet of things are gonna be larger than human centric data

01:00:36.560 --> 01:00:42.800
data that the real human is creating and one of the key problems that we face now is how can I

01:00:42.800 --> 01:00:49.680
interact with all this big data we have a dashboard on my tablet or my smartphone but it's too small

01:00:50.480 --> 01:00:56.720
we have a NLP natural language processing algorithms that I can speak to as smart device and have

01:00:56.720 --> 01:01:03.040
access to this big data or I can interact with smart devices like this thermostat and I can see the

01:01:03.040 --> 01:01:12.320
data but one of the most expected breakthrough that is gonna appear is through the use of AR and VR

01:01:12.320 --> 01:01:20.160
I'm gonna be able to visualize big data on the physical world by connecting AR applications with

01:01:20.160 --> 01:01:29.120
internet of things networks so accessibility to data is gonna be an immersive experience to us

01:01:29.120 --> 01:01:31.680
instead of having let's say a flat screen in front of us

01:01:35.520 --> 01:01:42.960
that's all on my side and obviously there is a list of conclusions that you can see in your slide

01:01:42.960 --> 01:01:50.240
and George we can welcome questions and I hope you found the lecture interesting thank you very

01:01:50.240 --> 01:01:56.960
much thank you very much George thank you very much Chris this was a really packed session but

01:01:57.760 --> 01:02:04.160
at least for me because I watched it more as a student because I'm not an expert in these things

01:02:04.160 --> 01:02:09.760
I found it very fascinating just to let everyone know that this is quite a long presentation you

01:02:09.760 --> 01:02:14.880
might have noticed that it's more than 70 slides so we're gonna mint it and have it available for

01:02:14.880 --> 01:02:22.320
you to claim as an NFT as soon as possible and obviously both Chris and George will be available

01:02:22.320 --> 01:02:30.720
for for questions offline as well on on biber or twitter so we have a couple of minutes I think

01:02:30.720 --> 01:02:40.080
we can take a couple of questions one question is okay people are naturally confused with acronyms

01:02:40.080 --> 01:02:46.880
so George you started by trying to explain the differences between AR VR and MR

01:02:47.920 --> 01:02:51.840
student asking about XR which is your standard reality

01:02:52.560 --> 01:02:58.560
I guess I know the answer to that question but can you clarify the difference on how XR fits with

01:02:58.560 --> 01:03:07.120
the other acronyms and what everything is yeah acronyms are and abbreviations are always a big

01:03:07.120 --> 01:03:20.080
issue and sometimes there is an overlap extended reality mixed reality AR VR I think that we are

01:03:20.080 --> 01:03:30.000
gonna have a more dominant let's say names focusing on VR everything that has to do without any

01:03:30.000 --> 01:03:36.800
type of interaction in the physical world so I'm totally isolated in a virtual experience and then

01:03:36.800 --> 01:03:42.720
XR I think in my personal opinion you know that will include all the rest but this is something

01:03:42.720 --> 01:03:49.120
that you know we're gonna see different names probably coming in the near future so me personally

01:03:49.120 --> 01:03:55.920
I use VR AR some other people are using XR so it's up to you to use the name that you prefer

01:03:57.840 --> 01:03:59.440
Chris any comment or comments?

01:03:59.440 --> 01:04:05.040
Chris might be able to provide well I use VR for everything

01:04:08.960 --> 01:04:16.800
okay case is boy yeah I personally like to keep it simple and I I just say well it's it's

01:04:16.800 --> 01:04:26.880
I think virtual reality is good enough if it's going to blend with you know but who's to say what

01:04:26.880 --> 01:04:35.040
real reality is anyway so keep it simple virtual reality is fine XR I have read papers

01:04:36.560 --> 01:04:46.320
which say just treat the X as a as a variable just a placeholder so in the X you can you can

01:04:46.320 --> 01:04:54.160
put whatever or mentored you can put the glasses you can put immersive and whatever comes next you

01:04:54.160 --> 01:05:04.000
know so I'd rather not confuse people I'd rather not confuse people and I would either just go

01:05:04.000 --> 01:05:11.200
with VR or go with with what George just said AR and VR are fine I mean it's it's good enough

01:05:11.200 --> 01:05:20.640
yeah I agree I mean the the keep it simple I think principle applies here I'm probably older than

01:05:20.640 --> 01:05:26.240
everyone around here and I've been around in the early days of the internet the early days of

01:05:26.240 --> 01:05:36.000
mobile the early days of crypto and I've seen how acronyms are used and abused by consultants

01:05:36.000 --> 01:05:41.840
and vendors as they try to to position their products and differentiate themselves from

01:05:41.840 --> 01:05:49.120
competition so sometimes we get you know bombarded with different acronyms that mostly mean if not

01:05:49.120 --> 01:05:55.200
completely the same very similar things and tends to be confusing so yeah I'm only in for simplicity

01:05:56.480 --> 01:06:02.480
and you know as it happened with the internet the things that have real value that the names

01:06:02.480 --> 01:06:08.000
will stick others like you know the internet we have been discussing back then in the 90s or

01:06:08.000 --> 01:06:14.080
everything will just disappear from the from the foreground okay another question

01:06:15.920 --> 01:06:22.000
both of you especially George I think have mentioned a number of of of devices that are

01:06:22.640 --> 01:06:30.080
commercially available announced or in the process of being developed and okay we I guess most of us

01:06:30.080 --> 01:06:36.640
know about occlusion stuff like that but you mentioned things like haptic interfaces or treadmills

01:06:36.640 --> 01:06:43.920
or this actuator in the year that will alleviate the symptoms of motion sickness can you give us

01:06:43.920 --> 01:06:54.320
either of you a like a a time horizon of when these things would hit the commercial market when

01:06:54.320 --> 01:07:00.080
we would see them I mean are they available in the market now are we expecting them in 2023 or is

01:07:00.080 --> 01:07:12.640
it like a five year horizon thing resource would I go first yes go okay the technology is already

01:07:12.640 --> 01:07:21.360
here and obviously there is a supply and demand you know driver here so the more the demand is

01:07:21.360 --> 01:07:27.680
gonna grow from the end users the technology will accelerate we have seen cases that were the

01:07:27.680 --> 01:07:32.160
technology accelerated so fast but the user adoption was not there and this from the business

01:07:32.160 --> 01:07:39.600
perspective is you know sometimes not very sustainable but for the moment technology is here to deliver

01:07:39.600 --> 01:07:48.480
you know acceptable levels of immersion and experience so it can be engaging for the end user so gloves

01:07:48.480 --> 01:07:58.640
that can improve let's say haptic VR okay or treadmills and they already exist they might be hard to find

01:07:58.640 --> 01:08:04.480
because there is no mass production there are no games you know still yet out there to let you

01:08:04.480 --> 01:08:11.440
experience you know with the use of a haptic glove you know the level of immersion that you want so

01:08:11.440 --> 01:08:16.960
there is the technologies here the demand is coming so we are gonna see like a step-by-step

01:08:16.960 --> 01:08:27.680
growth my personal sense is that you know 2023 we are gonna see much more evolution compared to 22

01:08:29.680 --> 01:08:34.000
and more penetration of this type of technologies in our experiences

01:08:34.000 --> 01:08:47.360
yeah so yeah I tend to agree but if you ask me which one of the AR VR is going to hit

01:08:48.080 --> 01:08:55.440
a use case or a use scenario quicker I think it's going to be augmented reality because of the

01:08:55.440 --> 01:09:04.720
you know not everybody as George mentioned some people really do not like the sense of isolation

01:09:04.720 --> 01:09:13.920
that you get from immersive tech you know and I've been working with the tech for quite a long time

01:09:13.920 --> 01:09:20.800
and yeah you don't find me putting on my headset I'd like to kick back and watch a nice flat screen

01:09:20.800 --> 01:09:32.960
but imagine this you've got augmented reality glasses and you kick back and you turn your room

01:09:32.960 --> 01:09:47.360
into a living cinema now this is a use case it flows with larger and larger TV screens for example

01:09:47.360 --> 01:09:54.960
that everywhere every Christmas you're buying a bigger TV set well at some point you don't need

01:09:54.960 --> 01:10:02.000
to buy a TV set okay you can have a shared experience with your family wearing a pair of glasses that

01:10:02.000 --> 01:10:07.760
you can take with you from one room to the next there could be a market for this employment

01:10:07.760 --> 01:10:17.360
entertainment in terms of home entertainment for business use is absolutely you know everything is

01:10:17.360 --> 01:10:30.720
there currently it will get better haptics has fallen out it's fallen out a little bit because

01:10:30.720 --> 01:10:41.840
you know it's clunky it's to the technology is too clunky to be viable at the moment I remember the

01:10:41.840 --> 01:10:53.600
the old haptic device was called the phantom if anyone would like to go back to the 1990s the late

01:10:53.600 --> 01:11:02.000
1990s early 2000s so this was a little robot you put your finger into it and you could feel

01:11:02.720 --> 01:11:08.800
you could feel stuff and then and play around with you know elastic effects etc

01:11:11.760 --> 01:11:19.920
now we have haptic gloves but I think yeah this will be a while taking off I think it's a slow

01:11:19.920 --> 01:11:25.920
process let me let me take you a little bit further in the future then because I have a question that

01:11:25.920 --> 01:11:32.240
I really like from one of our students and the question is what about brain computer interfaces

01:11:32.240 --> 01:11:38.880
how far away is that do you think I guess eventually we will tap directly into the optical part of the

01:11:38.880 --> 01:11:46.960
brain and bypass AR spectacle sore goggles do you have any views on this that's already here

01:11:46.960 --> 01:11:57.840
is it okay yeah yeah that's that's already here so okay there is a principle I have one just here

01:11:57.840 --> 01:12:08.240
and in fact it's a 32 channel BCI with a principal shell so you could you get the 3d model you

01:12:08.240 --> 01:12:16.160
could you can print it and you  get a pack from a open BCI is the name of the company it was a kick

01:12:16.160 --> 01:12:24.720
starter from a few years ago and the perfect use case is as a motion device

01:12:24.720 --> 01:12:40.480
yeah so you can train it picks up the skin currents on the head on the on the scalp you can train

01:12:40.480 --> 01:12:49.280
it on the motor the sensors of the head with repetitive movements and then you can associate

01:12:49.280 --> 01:12:54.560
those movements with movement in a virtual environment and people have been doing that for

01:12:54.560 --> 01:13:05.440
a few years now yeah and there's also it's used for paraplegics so if you have a case where you use

01:13:05.440 --> 01:13:11.680
it for a paraplegic in a wheelchair to move their wheelchair and you can for sure take this straight

01:13:11.680 --> 01:13:19.600
away and put it into a virtual environment fascinating I didn't know we were so advanced in BCI next

01:13:19.600 --> 01:13:28.000
time I am in your lab you you need to show me this George  and he used a lot and I think that

01:13:28.000 --> 01:13:36.400
one of the enablers of something really really interesting is gonna be 5g or you know 66g networks

01:13:36.400 --> 01:13:46.480
that will allow real-time 360 video transfer and I remember that a couple of years ago when I

01:13:46.480 --> 01:13:55.920
broke my leg all right I said I would pay anything if I could click on a person on a map let's say

01:13:57.440 --> 01:14:04.640
on the top of a mountain that he or she is no boring with a 360 camera on her head okay and I can be

01:14:04.640 --> 01:14:12.800
with my broken leg in my sofa of my home in Greece and wear my VR headset and you have the assert

01:14:12.800 --> 01:14:20.640
experience real-time 360 high-definition video and the person is you know doing a nice downhill

01:14:20.640 --> 01:14:30.000
run for me while I can't so I think that when we are gonna have a content creators real-time

01:14:30.800 --> 01:14:38.800
high-definition 360 video be able to be transferred and headsets that can allow us to you know consume

01:14:38.800 --> 01:14:45.200
this type of content there's gonna be a really really interesting application area

01:14:46.720 --> 01:14:54.560
we are a little bit some years behind because the network and the speed is not already there

01:14:54.560 --> 01:15:00.960
in many cases in some you know denser burn environments it is but I think this is gonna be

01:15:00.960 --> 01:15:04.880
fascinating and obviously we need the 360 cameras in our smartphones

01:15:04.880 --> 01:15:12.880
awesome very very interesting once you then did asking what was the name of the company that you

01:15:12.880 --> 01:15:20.160
mentioned Greece I think it was open BCI you said you're muted you're muted I think

01:15:21.840 --> 01:15:28.240
open BCI open BCI for brain computer interface great yeah okay

01:15:28.240 --> 01:15:39.040
another question is if you were to pick like one or the top difficulty either technical or

01:15:39.040 --> 01:15:47.760
adoption related or regulatory or whatever you want to make these things you know commercially

01:15:47.760 --> 01:15:55.120
viable and adopted by NMASS what do you think that the biggest obstacles or obstacles are at the

01:15:55.120 --> 01:16:01.680
moment is it that we are you know missing technological elements is it that we miss

01:16:02.640 --> 01:16:08.560
applications that we miss education what is it that hasn't allowed of my

01:16:08.560 --> 01:16:10.880
individual reality to reach their full potential

01:16:14.320 --> 01:16:19.440
I think there are different use cases for for each of them

01:16:19.440 --> 01:16:31.440
I believe and well I saw a breakdown of 60 to 40 on matters expenditure vis-a-vis the

01:16:32.000 --> 01:16:39.920
augmented reality expenditure versus virtual reality and I don't think it's the case that

01:16:39.920 --> 01:16:44.880
matter for example is building this closed world in the evening you know they expect this closed

01:16:44.880 --> 01:16:50.560
world this is not going to be the use case it's not it's not going to be the you know what's going

01:16:50.560 --> 01:17:04.640
to break it for for this tech I think this tech will gradually it become pervasive

01:17:05.440 --> 01:17:11.840
through everything that we do it it's a slow process and I don't think that there is going to be a

01:17:11.840 --> 01:17:21.200
massive jump in my personal opinion I think what will happen is that we will just I saw a

01:17:21.200 --> 01:17:27.680
visualization of this itself where somebody walks out of their living room and they are bombarded

01:17:28.400 --> 01:17:37.440
with augmentation right so in the streets where they walk there's information this information

01:17:37.440 --> 01:17:45.280
regarding the strict name there's advertising once the advertisers get in there oh believe me

01:17:46.000 --> 01:17:53.680
things will take off people I also saw in our cafeteria today a pair of glasses which

01:17:54.640 --> 01:18:00.480
you know stop the glare from a screen and I think the wearing of glasses like this with a form

01:18:00.480 --> 01:18:11.440
factor like this with computer graphics augmented is going to be the clinician people will start

01:18:11.440 --> 01:18:20.560
wearing these they'll be tough to wear in them and we will have everywhere we go there will be data

01:18:20.560 --> 01:18:28.880
it will be data rich and this will be the most of us in the in my opinion very interesting very

01:18:28.880 --> 01:18:34.480
interesting definition of the metaverse as well jolts and any final thoughts on this because I

01:18:34.480 --> 01:18:42.320
think this is this is our last question for for today I agree that you know one of the key obstacles

01:18:42.320 --> 01:18:48.960
you know to wear the glasses is because now we send all the processing power on top of the glass

01:18:48.960 --> 01:18:55.200
what we need is a glass that it is like the one that we wear for our son or you know to improve

01:18:55.200 --> 01:19:02.720
our site so having migrating all the processing power to another device probably our smartphone

01:19:02.720 --> 01:19:13.680
our you know our smart words and having the acceptable level of graphics and experience

01:19:13.680 --> 01:19:19.680
and delivered in a normal class will open new horizons in the adoption of the services and then

01:19:19.680 --> 01:19:27.360
you know wherever you are you can see you'll commit information everywhere advertising you know real

01:19:27.360 --> 01:19:34.880
time information navigation everything can make your life much easier fantastic we're gonna say

01:19:34.880 --> 01:19:41.360
we're gonna say huge stages in the coming years and I agree with Chris that they would happen gradually

01:19:41.360 --> 01:19:50.960
and then and then suddenly maybe when advertisers pick up on on these and we have a sudden influx of

01:19:50.960 --> 01:19:56.880
of applications all around us and then we will be chasing the applications that of them chasing us

01:19:57.440 --> 01:20:02.720
anyway thank you very much this was a very fascinating session thank you for being here thank you for

01:20:02.720 --> 01:20:08.880
sharing your expertise with us and looking forward to seeing you again in one of our future courses

01:20:08.880 --> 01:20:15.680
thank you very much everyone we'll see you next week with week then bye bye bye bye

01:20:38.880 --> 01:20:41.440
bye

